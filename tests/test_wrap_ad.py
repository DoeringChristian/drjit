import drjit as dr
import pytest
import warnings

try:
    # Ignore deprecation warnings generated by the PyTorch package
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore",category=DeprecationWarning)
        import torch

        # The following operation can trigger a deprecation warning, so let's trigger
        # it here once to silence the warning instead of putting the condition into
        # every test.
        import torch.autograd.forward_ad as fwd_ad
        with fwd_ad.dual_level():
            fwd_ad.make_dual(torch.tensor(1.0), torch.tensor(1.0))

    pytorch_missing = False
except ImportError:
    pytorch_missing = True
warnings.filterwarnings("error")

needs_pytorch = pytest.mark.skipif(pytorch_missing, reason="Test requires PyTorch")

@needs_pytorch
@pytest.test_arrays('is_diff,float,shape=(*)')
def test01_simple_bwd_target_torch(t):
    @dr.wrap_ad(source="drjit", target="torch")
    def foo(x):
        return x * 2

    x = dr.arange(t, 3)
    dr.enable_grad(x)
    y = foo(x)

    y.grad = [10, 20, 30]
    dr.backward_to(x)
    assert dr.all(y == [0, 2, 4])
    assert dr.all(x.grad == [20, 40, 60])


@needs_pytorch
@pytest.test_arrays('is_diff,float,shape=(*)')
def test02_simple_fwd_target_torch(t):
    @dr.wrap_ad(source="drjit", target="torch")
    def foo(x):
        return x * 2

    x = dr.arange(t, 3)
    dr.enable_grad(x)
    y = foo(x)

    x.grad = [10, 20, 30]
    dr.forward_to(y)
    assert dr.all(y == [0, 2, 4])
    assert dr.all(y.grad == [20, 40, 60])


@needs_pytorch
@pytest.test_arrays('is_diff,float,shape=(*)')
def test03_copy_mixed_types_multiarg_bwd_target_torch(t):
    @dr.wrap_ad(source="drjit", target="torch")
    def foo(x, y, a, b, foo='bar'):
        return y, x, x, a+b

    x = dr.arange(t, 3)
    y = t(4)
    dr.enable_grad(x, y)
    r1, r2, r3, r4 = foo(x=x, y=y, a=5, b=6)

    tt = dr.tensor_t(t)
    assert type(r1) is tt and type(r2) is tt and type(r3) is tt and type(r4) is int
    assert dr.all(r1 == y) and dr.all(r2 == x) and dr.all(r3 == x) and r4 == 11

    r1.grad = [10]
    r2.grad = [100, 200, 300]
    r3.grad = [1000, 2000, 3000]
    g1, g2 = dr.backward_to(x, y)

    assert dr.all(g1 == [1100, 2200, 3300])
    assert dr.all(g2 == [10])


@needs_pytorch
@pytest.test_arrays('is_diff,float,shape=(*)')
def test04_copy_mixed_types_multiarg_fwd_target_torch(t):
    @dr.wrap_ad(source="drjit", target="torch")
    def foo(x, y, a, b, foo='bar'):
        return y, x, x, a+b

    x = dr.arange(t, 3)
    y = t(4)
    dr.enable_grad(x, y)
    r1, r2, r3, r4 = foo(x=x, y=y, a=5, b=6)

    tt = dr.tensor_t(t)
    assert type(r1) is tt and type(r2) is tt and type(r3) is tt and type(r4) is int
    assert dr.all(r1 == y) and dr.all(r2 == x) and dr.all(r3 == x) and r4 == 11

    x.grad = [10, 20, 30]
    y.grad = [40]
    g1, g2, g3 = dr.forward_to(r1, r2, r3)

    assert dr.all(g1 == [40])
    assert dr.all(g2 == [10, 20, 30])
    assert dr.all(g3 == [10, 20, 30])


@needs_pytorch
@pytest.test_arrays('is_diff,float,shape=(*)')
def test05_args_kwargs_fwd_target_torch(t):
    @dr.wrap_ad(source="drjit", target="torch")
    def foo(*args, **kwargs):
        return args[0] * kwargs["y"]

    x = dr.arange(t, 3)
    y = t(4)
    dr.enable_grad(x, y)
    r = foo(x, y=y)

    x.grad = [10, 20, 30]
    y.grad = [40]
    g = dr.forward_to(r)

    assert dr.all(g == [40, 120, 200])


@needs_pytorch
@pytest.test_arrays('is_diff,float,shape=(*)')
def test05_args_kwargs_fwd_target_torch(t):
    @dr.wrap_ad(source="drjit", target="torch")
    def foo(*args, **kwargs):
        return args[0] * kwargs["y"]

    x = dr.arange(t, 3)
    y = t(4)
    dr.enable_grad(x, y)
    r = foo(x, y=y)

    x.grad = [10, 20, 30]
    y.grad = [40]
    g = dr.forward_to(r)

    assert dr.all(g == [40, 120, 200])
